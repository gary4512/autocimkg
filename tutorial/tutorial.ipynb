{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **AutoCimKG: Tutorial**",
   "id": "65beabec31559993"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from autocimkg import AutoCimKGCore\n",
    "from autocimkg import DocumentsDistiller\n",
    "from autocimkg.models import Document, Employee, Ontology, KnowledgeGraph, KnowledgeGraphVersion\n",
    "from autocimkg.graph_integration import GraphIntegrator\n",
    "from autocimkg.metadata_integration import MetadataIntegrator\n",
    "from autocimkg.utils import ScientificArticle, AuthorsOnly"
   ],
   "id": "f0aeddaaed5af719",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **initialisation of models, import of required data and pre-processing**  \n",
    "The first step is to initialise the chat and embedding model with the respective API token(s)."
   ],
   "id": "8ada265e2cf04b5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "openai_api_key = \"\"\n",
    "\n",
    "openai_llm_model = ChatOpenAI(\n",
    "    api_key = openai_api_key,\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    model_kwargs={\"response_format\": {\"type\": \"json_object\"}}\n",
    ")\n",
    "\n",
    "openai_embeddings_model = OpenAIEmbeddings(\n",
    "    api_key = openai_api_key ,\n",
    "    model=\"text-embedding-3-large\"\n",
    ")"
   ],
   "id": "d3f839d26802e22f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, the raw documents are fetched from the workspace of the project. This requires specifying the path (e.g. '../tutorial/data/abc.pdf'), possible pages to exclude and a type (e.g. a 'scientific article') for each document to process. ",
   "id": "5a1ae17364a5e333"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "raw_documents = []\n",
    "documents_to_fetch = [\n",
    "    (\"../tutorial/data/abc.pdf\", list(range(2,50,1)), 'scientific article'),\n",
    "    (\"../tutorial/data/def.pdf\", list(range(2,50,1)), 'scientific article')\n",
    "]\n",
    "\n",
    "for path_, exclude_pages, type_ in documents_to_fetch:\n",
    "    loader = PyPDFLoader(path_)\n",
    "    pages = loader.load_and_split()\n",
    "    pages = [page for page in pages if page.metadata[\"page\"]+1 not in exclude_pages] # exclude some pages (unnecessary pages, e.g. the references)\n",
    "    pages = [page.page_content.replace(\"{\", '[').replace(\"}\", \"]\") \n",
    "             for page in pages]\n",
    "    head, tail = os.path.split(path_)\n",
    "    raw_documents.append({'name': tail, 'content': pages, \n",
    "                          'extract_type': type_, 'doc_type': type_})"
   ],
   "id": "2008e1bdfa3e55f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Optionally, the raw documents can be compressed to blocks. Here, the LLM uses a user-defined blueprint to extract certain parts of the full-text (e.g. title, abstract, keywords and authors).",
   "id": "7d58996a1db34167"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "documents: list[Document] = []\n",
    "document_distiller = DocumentsDistiller(llm_model=openai_llm_model)\n",
    "\n",
    "for raw_document in raw_documents:\n",
    "    \n",
    "    # determine blueprint\n",
    "    blueprint = None\n",
    "    if raw_document['extract_type'] == 'scientific article': \n",
    "        blueprint = ScientificArticle\n",
    "    elif raw_document['extract_type'] == 'authors only': \n",
    "        blueprint = AuthorsOnly\n",
    "    else: \n",
    "        print(f\"No template for document distillation found for {raw_document['extract_type']}\")\n",
    "        continue\n",
    "        \n",
    "    # determine context\n",
    "    context = ['\\n\\n'.join(raw_document['content'])] # list w/ one element per page => one-element list \n",
    "    if blueprint == AuthorsOnly: context = ['\\n\\n'.join(raw_document['content'][:2])] # AUTHORS ONLY: only first pages\n",
    "    \n",
    "    # distill\n",
    "    distilled_doc = document_distiller.distill(\n",
    "            documents = context,\n",
    "            document_type = raw_document['doc_type'],\n",
    "            output_data_structure = blueprint\n",
    "    )\n",
    "    \n",
    "    # assemble\n",
    "    content = [f\"{raw_document['doc_type']}'s {key} - {value}\".replace(\"{\", \"[\").replace(\"}\", \"]\") \n",
    "               for key, value in distilled_doc.items() \n",
    "               if value and value != [] and key != \"authors\"]\n",
    "    if not content: content = raw_document['content'] # AUTHORS ONLY: take whole content\n",
    "    # spare authors from being processed later on!\n",
    "    authors = distilled_doc.get(\"authors\") if \"authors\" in distilled_doc else []\n",
    "\n",
    "    document = Document(name = raw_document['name'], doc_type = raw_document['doc_type'], content = content, authors = authors, language = \"eng\")\n",
    "    # create / save metadata!\n",
    "    \n",
    "    documents.append(document)"
   ],
   "id": "3a1cf9b324d8b56b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lastly, the relational master data need to be fetched and preprocessed. AutoCimKG expects a CSV file with the following values for each person: ID, GIVENNAME, SURNAME, COMPANY, DEPARTMENT and STATUS.",
   "id": "8ffc904d1a321f06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "employees_df = pd.read_csv('../tutorial/data/employee.csv', sep=\";\", header='infer')\n",
    "employees_df['NAME'] = employees_df.apply(lambda employee: employee['SURNAME'] + ' ' + employee['GIVENNAME'], axis=1)\n",
    "# resolution based on full, lowercase name\n",
    "employees_df['NAME_EMBEDDING'] = employees_df.apply(lambda employee: np.array(openai_embeddings_model.embed_query(employee['NAME'].lower())), axis=1)\n",
    "\n",
    "employees = [Employee(id_=empl[0], name=empl[1], name_embedding=empl[5], company=empl[2], department=empl[3], status=True if empl[4] == \"active\" else False) for empl in zip(employees_df['ID'], employees_df['NAME'], employees_df['COMPANY'], employees_df['DEPARTMENT'], employees_df['STATUS'], employees_df['NAME_EMBEDDING'])]"
   ],
   "id": "576d56aa7f0fa046",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **KG construction and maintenance**  \n",
    "Now, the actual creation and upkeep of the competency KG can be initiated. Here, AutoCimKG offers many options for parametrisation. Moreover, this step consumes the prepared documents, the relational master data and optionally a lightweight ontology to align the whole text-based extraction. If a competency KG already exists and needs to be incrementally maintained with new documents, it would be handed over to the corresponding function call here. The source code provides insights in all offered options."
   ],
   "id": "23d673d6b77ef3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start_proc_ts = datetime.now()\n",
    "\n",
    "ont = Ontology(topics=[{\"Subject area\": \"Concise description of a domain-specific area.\"}],\n",
    "    relations=[{\"Relationship type\": \"Concise description of a desired relation type between a START and END entity.\"}],\n",
    "    strict=False)\n",
    "\n",
    "autocimkg = AutoCimKGCore(llm_model = openai_llm_model, embeddings_model = openai_embeddings_model)\n",
    "kg, ont = autocimkg.build_graph(ontology = ont, \n",
    "                            documents = documents, employees = employees, domain = \"financial supervisory domain\", expert_threshold=0.8, ent_threshold = 0.8, \n",
    "                            rel_threshold = 0.8, max_tries_isolated_entities=0)\n",
    "\n",
    "# STOP KGC\n",
    "log = autocimkg.log[0]\n",
    "conf = autocimkg.conf[0]\n",
    "agent = autocimkg.agent\n",
    "chat_conf = autocimkg.llm_model.model_dump()\n",
    "chat_conf['openai_api_key'] = \"\"\n",
    "emd_conf = autocimkg.embeddings_model.model_dump()\n",
    "emd_conf['openai_api_key'] = \"\"\n",
    "\n",
    "end_proc_ts = datetime.now()"
   ],
   "id": "518399bdf02069e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **database integration**  \n",
    "After successful construction or maintenance of a competency KG, AutoCimKG can be used to store and retrieve graph and metadata with respect to a connected database."
   ],
   "id": "735a374e870191e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "HOST = \"\"\n",
    "PORT = 0\n",
    "DBNAME = \"\"\n",
    "USERNAME = \"\"\n",
    "PASSWORD = \"\"\n",
    "\n",
    "GRAPH = \"kg_v1\"\n",
    "\n",
    "graph_integrator = GraphIntegrator(host = HOST, port = PORT, dbname = DBNAME, username = USERNAME, password = PASSWORD)\n",
    "metadata_integrator = MetadataIntegrator(host = HOST, port = PORT, dbname = DBNAME, username = USERNAME, password = PASSWORD)"
   ],
   "id": "65d7c42172498440",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "graph_integrator.delete_graph(GRAPH)\n",
    "graph_integrator.write_graph(GRAPH, kg)\n",
    "kg_db = graph_integrator.read_graph(GRAPH)"
   ],
   "id": "b753d6b5e45ae19a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metadata_integrator.init_db()",
   "id": "a6a26b0d304cbdc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metadata_integrator.create_kg_version(KnowledgeGraphVersion(kg_name=GRAPH, agent=agent, start_proc_ts=start_proc_ts, end_proc_ts=end_proc_ts))\n",
    "for kg_version in metadata_integrator.read_kg_versions():\n",
    "    print(kg_version.kg_name, kg_version.agent, kg_version.start_proc_ts, kg_version.end_proc_ts)\n",
    "# metadata_integrator.delete_kg_version(GRAPH)"
   ],
   "id": "d90a0a2c0a308d0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metadata_integrator.create_logs(kg_name=GRAPH, logs=log)\n",
    "\n",
    "for entry in metadata_integrator.read_logs(kg_name= GRAPH):\n",
    "    print(entry.ts, entry.logger_name, entry.log_level, entry.message)\n",
    "# metadata_integrator.delete_logs(GRAPH)"
   ],
   "id": "570c20e3cdbc48b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metadata_integrator.create_data_sources(documents=documents)\n",
    "\n",
    "for doc in metadata_integrator.read_data_sources():\n",
    "    print(doc.name, doc.language, doc.authors, doc.doc_type)\n",
    "# metadata_integrator.delete_data_sources()"
   ],
   "id": "f3ae8d902d872ce8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metadata_integrator.create_ontology(GRAPH, ont)\n",
    "\n",
    "for ontology in metadata_integrator.read_ontologies(GRAPH):\n",
    "    print(ontology.topics, ontology.relations, ontology.strict)\n",
    "# metadata_integrator.delete_ontologies(GRAPH)"
   ],
   "id": "35d18e41add0dd3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metadata_integrator.create_llm_config(kg_name=GRAPH, llm_config=chat_conf)\n",
    "metadata_integrator.create_llm_config(kg_name=GRAPH, llm_config=emd_conf)\n",
    "\n",
    "for config in metadata_integrator.read_llm_configs(GRAPH):\n",
    "    print(config)\n",
    "# metadata_integrator.delete_llm_configs(GRAPH)"
   ],
   "id": "dc82e342e6a8235",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metadata_integrator.create_autocimkg_config(GRAPH, conf)\n",
    "\n",
    "for config in metadata_integrator.read_autocimkg_configs(GRAPH):\n",
    "    print(config)\n",
    "# metadata_integrator.delete_autocimkg_configs(GRAPH)"
   ],
   "id": "c5ac0d32a6fa9e13",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
